from nltk.tokenize import word_tokenize from nltk.corpus import stopwords
# Example documents documents = [
"The quick brown fox jumped over the lazy dog", "The lazy dog slept in the sun",
"The quick brown dog chased the squirrel up the tree", "The sun shines bright in the sky",
"The brown fox is quick and agile"
]
# Tokenize and create the inverted index def build_inverted_index(documents):
inverted_index = {}
stop_words = set(stopwords.words('english'))
for doc_id, document in enumerate(documents, start=1):
words = set(word_tokenize(document.lower())) - stop_words for word in words:
if word not in inverted_index: inverted_index[word] = []
inverted_index[word].append(doc_id) return inverted_index
# Boolean AND operation using inverted index def boolean_and(operands, index, total_docs):
if not operands:
return list(range(1, total_docs + 1))
result = set(index.get(operands[0], set())) for term in operands[1:]:
result = result.intersection(index.get(term, set())) return list(result)
# Boolean OR operation using inverted index def boolean_or(operands, index):
result = set()
 
for term in operands:
result = result.union(index.get(term, set())) return list(result)
# Boolean NOT operation using inverted index def boolean_not(operand, index, total_docs):
operand_set = set(index.get(operand, set())) all_docs_set = set(range(1, total_docs + 1))
not_result = all_docs_set.difference(operand_set) return list(not_result)
# Tokenize the user query and perform boolean operation based on the input def perform_boolean_operation(query, inverted_index, total_docs):
query_terms = word_tokenize(query.lower())

if 'and' in query_terms:
operands = query.split(' AND ')
query_result = boolean_and(operands, inverted_index, total_docs) elif 'or' in query_terms:
operands = query.split(' OR ')
query_result = boolean_or(operands, inverted_index) elif 'not' in query_terms:
operand = query.split('NOT ')[1]
query_result = boolean_not(operand, inverted_index, total_docs) else:
# Default to OR operation if no operator is specified operands = query_terms
query_result = boolean_or(operands, inverted_index) print(operands)
return query_result
# Example usage
inverted_index = build_inverted_index(documents) total_docs = len(documents)

# Take query input from the user
query = input("Enter your boolean query: ")
# Perform boolean operation based on the user query
query_result = perform_boolean_operation(query, inverted_index, total_docs) print(f"Documents matching the query '{query}': {query_result}")












Given a query and a corpus of text documents, to rank the documents based on their similarity to the query. The relevance of a document should be determined by its similarity to the query, computed using the cosine similarity metric.
Program

from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise import cosine_similarity from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
# Example documents documents = [
"The quick brown fox jumped over the lazy dog", "The lazy dog slept in the sun",
"The quick brown dog chased the squirrel up the tree", "The sun shines bright in the sky",
"A brown fox is quick and agile", "The lazy cat slept on the windowsill"
]
# Take query input from the user query = input("Enter your query: ")
# Tokenize the documents and query stop_words = set(stopwords.words('english'))
tokenized_documents = [" ".join([word for word in word_tokenize(doc.lower()) if word.isalnum() and word not in stop_words]) for doc in documents] tokenized_query = " ".join([word for word in word_tokenize(query.lower()) if word.isalnum() and word not in stop_words])

# TF-IDF Vectorization vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(tokenized_documents) query_vector = vectorizer.transform([tokenized_query])
# Calculate cosine similarity between the query vector and document vectors cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()

# Display the cosine similarities print("Cosine Similarities:")
for i, similarity in enumerate(cosine_similarities, start=1): print(f"Document {i}: {similarity}")

# Rank documents based on cosine similarity ranked_indices = cosine_similarities.argsort()[::-1]

# Display the ranked documents
print("\nRanked Documents based on Similarity:") for index in ranked_indices:
print(f"Document {index + 1}: {documents[index]} (Similarity Score:
{cosine_similarities[index]})")
