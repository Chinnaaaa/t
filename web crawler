import requests
from bs4 import BeautifulSoup from urllib.parse import urljoin
from urllib.robotparser import RobotFileParser import time

def get_html(url): try:
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
response = requests.get(url, headers=headers) response.raise_for_status()
return response.text
except requests.exceptions.RequestException as err: print(f"Request Error: {err}")
return None

def load_robots_txt(url): try:
robots_url = urljoin(url, '/robots.txt') robots_content = get_html(robots_url) robot_parser = RobotFileParser()
if robots_content: robot_parser.parse(robots_content.split('\n'))
return robot_parser except Exception as e:
print(f"Error loading robots.txt: {e}") return None
 
def extract_links(html, base_url):
soup = BeautifulSoup(html, 'html.parser')
links = [urljoin(base_url, link['href']) for link in soup.find_all('a', href=True)]
return links

def is_allowed_by_robots(robot_parser, url): return robot_parser.can_fetch('*', url)

def crawl(start_url, max_depth=3, delay=1): visited_urls = set()
robot_parser = load_robots_txt(start_url)

def recursive_crawl(url, depth):
if depth > max_depth or url in visited_urls or not is_allowed_by_robots(robot_parser, url):
return visited_urls.add(url) time.sleep(delay) html = get_html(url) if html:
print(f"Crawling {url}")
links = extract_links(html, url) for link in links:
recursive_crawl(link, depth + 1)

if not robot_parser or not is_allowed_by_robots(robot_parser, start_url):
print(f"Access to {start_url} is restricted by robots.txt.
Crawling aborted.")
return recursive_crawl(start_url, 1)
