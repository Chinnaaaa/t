def calculate_precision_recall_f1(tp, fp, fn): precision = tp / (tp + fp) if (tp + fp) > 0 else 0 recall = tp / (tp + fn) if (tp + fn) > 0 else 0
f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
return precision, recall, f1
# User input for TP, FP, FN
tp = int(input("Enter the number of True Positives (TP): ")) fp = int(input("Enter the number of False Positives (FP): ")) fn = int(input("Enter the number of False Negatives (FN): "))
# Calculate precision, recall, and F1-score
precision, recall, f1 = calculate_precision_recall_f1(tp, fp, fn)
# Print the results print(f"Precision: {precision:.2f}") print(f"Recall: {recall:.2f}")
print(f"F1-score: {f1:.2f}")








  Calculate the precision, recall, and F-measure metrics for an information retrieval system based on the number of true positives (TP), false positives (FP), and false negatives (FN) obtained from comparing two sets of retrieved documents and relevant documents.
Program

def calculate_metrics(retrieved_set, relevant_set): true_positive = len(retrieved_set.intersection(relevant_set)) false_positive = len(retrieved_set.difference(relevant_set)) false_negative = len(relevant_set.difference(retrieved_set))

print("True Positive: ", true_positive, "\nFalse Positive: ", false_positive,
"\nFalse Negative: ", false_negative, "\n")
precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0
recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0
f_measure = 2 * precision * recall / (precision + recall) if (precision
+ recall) > 0 else 0
return precision, recall, f_measure # Create an exhaustive list of terms
all_terms = ["apple", "banana", "orange", "grape", "melon","papaya"] retrieved=["banana", "orange","papaya"] relevant=["orange","papaya","melom"]
# Example usage:
retrieved_set = set(retrieved) # Predicted set
relevant_set = set(relevant) # Actually Needed set (Relevant)

precision, recall, f_measure = calculate_metrics(retrieved_set, relevant_set)
print(f"Precision: {precision}") print(f"Recall: {recall}") print(f"F-measure: {f_measure}")













  Given a set of predicted binary labels (y_pred) and their corresponding true binary labels (y_true), evaluate the performance by calculating precision, recall, f1-score and average precision of a binary classification model using standard metrics from scikit-learn module
Program

from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, roc_auc_score

# Example data
y_true = [0, 1, 1, 0, 1]
y_scores = [0.1, 0.8, 0.6, 0.3, 0.9] # Predicted scores (probability of positive class)

# Calculate precision-recall curve
precision, recall, _ = precision_recall_curve(y_true, y_scores)

# Calculate average precision
avg_precision = average_precision_score(y_true, y_scores)


# Print the results print("Precision-Recall Curve:") print("Precision:", precision) print("Recall:", recall)
print("Average Precision:", avg_precision)
